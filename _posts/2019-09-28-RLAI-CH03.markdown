---
title: "RLAI-CH03"
data: 2019-9-27
tags: "RLAI"

---

## 第3章 有限马尔可夫决策过程

在本章中，我们将介绍有限马尔可夫决策过程或有限MDP的形式问题，我们将在本书的其余部分尝试解决这些问题。

MDP是顺序决策的经典形式化，其中行动不仅影响直接奖励，还影响后续情况或状态，以及贯穿未来的奖励。 因此，MDP涉及延迟奖励以及交换即时与延迟奖励的需要。

### 3.1 个体环境接口

MDP旨在直接构建从交互中学习以实现目标的问题。 学习者和决策者被称为 个体（agent）。
![图3.1：马尔可夫决策过程中的个体 - 环境交互。](https://rl.qiwihui.com/zh_CN/latest/_images/figure-3.1.png)

在 有限 MDP中，状态，动作和奖励 （S，A 和 R）的集合都具有有限数量的元素。

随机变量 Rt 和 St 具有明确定义的离散概率分布，仅取决于先前的状态和动作。

动力学函数 $p:S×R×S×A→[0,1]$ 是四个参数的普通确定性函数。 

St  和 Rt 的每个可能值的概率 仅取决于前一个状态和动作 St−1 和 At−1， 并且在给定它们的情况下，它们根本不依赖于先前的状态和动作。

最好将其视为对决策过程的限制，而不是对 状态 的限制。 状态必须包括有关过去的个体-环境交互的所有方面的信息，这些信息对未来有所影响。 如果确实如此，那么就说该状态拥有 马尔可夫性。

时间步长不需要指固定的实时间隔；它们可以指任意连续的决策和行动阶段。

 一般而言，行动可以是我们想要学习如何制定的任何决定，而状态可以是我们可以知道的任何可能有助于制作它们的任何事物。

奖励可以在自然和人工学习系统的物理体内计算，但被认为是个体的外部。

个体的边界，代表着个体绝对控制能力的限制，而不是限制它的知识。在复杂的机器人中，许多不同的个体可能同时运行，每个个体都有自己的边界。

表征性选择目前更多的是艺术而非科学，给出了表达状态和行为的好方法的建议和例子。但我们主要焦点的是一旦表示被确定，如何学习行为的一般原则。

- **例3.1：生物反应器**：确定生物反应器的瞬间温度温度和搅拌速率。动作可以是传递到下级控制系统的目标温度和目标搅拌速率，该控制系统又直接激活加热元件和马达以实现目标。状态可能是有可能被过滤和延迟热电偶和其他传感器读数，加上代表大桶和目标化学品成分的符号输入。 奖励可能是生物反应器产生有用化学品的速率的逐时测量。
- **例3.2：拾取和放置机器人**：使用强化学习来控制机器人手臂在重复拾取和放置任务中的运动。想要学习快速和平稳的运动， 对于成功拾取和放置的每个对象，奖励可能为+1。为了鼓励平稳移动，在每个时间步骤上，可以根据动作的瞬间“急动”给出小的负面奖励。
- **例3.3：环保机器人**： 当能量水平很 **高** 时，充电总是愚蠢的，所以我们不会将其包含在为此状态设定的动作中。r搜索>r等待，分别表示机器人在搜索和等待时将收集的预期罐数（以及预期的奖励）。 （只要搜索多了，收集罐数的期望就会升高，收集到一罐的时候就会回复到初始值上，类是明日方舟的抽卡方案。）

### 3.2 目标和奖励

个体的目标是最大化其收到的总奖励。这意味着最大化不是立即奖励，而是长期累积奖励。

奖励假说：所有我们所说的目标和目的都可以被认为是所接收的标量信号（称为奖励）的累积和的期望值的最大化。

 例如，国际象棋游戏个体应该仅仅因为实际获胜而获得奖励，而不是为了实现拿走对手的棋子或控制棋盘中心这样的子目标。 如果实现这些类型的子目标得到奖励，那么个体可能会找到一种方法来实现它们而不实现真正的目标。

奖励信号是你与机器人沟通的 *方式*，而不是您希望 *如何* 实现。



### 3.3 回报和情节

当个体-环境交互自然地分解为子序列时，我们称之为 *情节*（episode）。 每个情节在称为 *终点* 状态的特殊状态结束，随后是重置到标准起始状态或从起始状态的标准分布的抽样。

 *衰减因子*（discounting.）。根据这种方法，个体尝试选择动作，以使其在未来接收的衰减的奖励的总和最大化。对未来环境的影响。当衰减因子越大时（更接近1），汇报目标更加强烈地考虑了未来地回报。

- **示例3.4：杆平衡**：
  - 对于没有发生故障的每个时间步骤，奖励可以是+1，因此每次返回将是直到失败的步骤数。 
  -  在这种情况下，每次失败时奖励为1，其他时间奖励为零。然后每次返回 1−γK1−γK 与K相关，其中K是失败前的时间步数。

### 3.4 情节和持续任务的统一符号

情节任务：个体-环境交互自然地分解为一系列单独的情节

连续任务：应该就是不会停止的任务，不断交互，不会像情节任务一样有停止。

 情节任务在数学上更容易，因为每个动作仅影响在情节期间随后收到的有限数量的奖励。

### 3.5 策略和价值函数

估计状态（或状态-动作对）的 *价值函数*， 它们估计个体在给定状态下的 *好坏程度* （或者在给定状态下执行给定动作的程度有多好）。这里的“有多好”的概念是根据未来的奖励来定义的，或者准确的的说是预期回报方面。

如果为每个状态采取的每项行动保留单独的平均值，那么这些平均值将同样收敛于行动价值 qπ(s,a)。 我们称这种估计方法为 *蒙特卡洛方法*



### 3.6 最优策略和最优价值函数

解决强化学习任务大概意味着要从长远的角度找到一个取得很大回报策略。

### 3.7 优化和近似

对于我们感兴趣的各种任务，只能以极高的计算成本才能生成最优策略。

 强化学习的在线性质使得其有可能以更多的方式来近似最优策略，以便为经常遇到的状态作出良好的决策，而不用花费很少的努力来处理不经常遇到的状态。

### 3.8 总结

强化学习是从互动中学习如何行动从而实现目标。

其接口的规范定义了一个特定的任务：*动作* 是由个体所做的选择；*状态* 是做出选择的基础；*奖励* 是评估选择的基础。 个体内的一切都是由个体完全知晓和控制的；外面的一切都是不完全可控的，可能完全知道也可能不完全知道的。 *策略* 是随机规则，个体通过该规则选择动作作为状态的函数。 个体的目标是随着时间的推移最大限度地获得奖励。

当上述强化学习设置用明确定义的转移概率表示时，它构成马尔可夫决策过程（MDP）。 有限MDP是具有有限状态，动作和（当我们在此处制定）奖励集的MDP。 

