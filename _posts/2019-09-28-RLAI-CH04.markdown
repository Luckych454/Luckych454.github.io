---
title: "RLAI-CH04"
data: 2019-9-30
tags: "RLAI"

---

---
title: "RLAI-CH04"
data: 2019-9-30
tags: "RLAI"


---

## 动态规划

动态规划（DP）这个术语是指可以用于在给定完整的环境模型是马尔可夫决策过程（MDP）的情况下计算最优策略的算法集合。

事实上，那些方法都可以被看作是取得与DP算法相同的效果的尝试， 所不同的是这些算法需要比较少的计算量，并且不用假设理想的环境模型。

$v_*(s) &= \max_a\mathbb{E}[R_{t+1}+\gamma v_*(S_{t+1}) | S_t=s,A_t=a] \\&= \max_a\sum_{s',r}p(s',r|s,a)[r+\gamma v_*(s')]$

### 4.1 策略评估（预测）

任意策略pi 怎样计算状态值函数 value_pi。 这在DP文献中被称作 策略评估。我们把它当作 预测问题。

使用贝尔曼方程迭代的方法，直到找到与实际情况最接近的每一个状态的值。

环境动力学模型

### 4.2 策略提升

假设我们已经确定了一个任意确定性的策略pi价值函数value-pi。

就是说如果同时有几个动作都能获得最大值，那么随机策略就不需要从中选择一个单独的动作，取而代之的是，每一个取得最大值的动作在新的贪婪策略中有一定概率被选择。只要非最大动作的概率为零，任何分摊的方案都可以。



### 4.3 策略迭代

一旦策略pi已经在当时的value下提升为更好的policy，那么我们就可以在计算value来得到更好的policy。

策略迭代  = 策略评估+策略提升

**例4.2: 杰克租车**：将这个问题当作连续有限MDP，时间步骤是天数， 状态是每天结束是在每个位置剩余车子的数量，动作是每晚将车子在两个地点转移的净数量。

### 4.4 价值迭代

策略迭代的一个缺点是每一次迭代过程都包含策略评估，策略评估本身就可能是需要多次对整个状态集迭代计算的过程。 

一个重要的特例就是策略评估在一次迭代之后就停止（每个状态只有一个回溯）。这个算法就叫做 *价值迭代* 。

**例4.3：赌徒问题**：

### 4.5 异步动态规划

到目前为止我们所讨论的DP方法一个主要的缺点是他们涉及整个MDP状态集合，也就是说，需要对整个状态集合进行更新。如果状态集非常大，即使一次更新也会代价很大。

异步算法也使得与实时交互计算的结合更加容易。解决一个MDP问题，我们可以 *在个体真正经历MDP的同时* 运行迭代DP算法。



### 4.6 广义策略迭代

策略迭代包含两个同时进行的交互过程，一个使得价值函数与当前策略一致（策略评估），另一个使得策略在当前价值函数下变得贪婪（策略提升）。

我们用术语 *广义策略迭代* （GPI）这个词来指代策略评估和策略提升相互交互的一般概念，而不依赖于两个过程的粒度和其他细节。

 策略总是被价值函数进行更新

GPI中的评估和提升的过程可以认为是既存在竞争又存在合作。在竞争这个意义上他们走向相反的方向。 使价值函数的策略贪婪通常会使更改的策略的价值函数不正确，使价值函数与策略一致通常会导致该策略不再贪婪。

 任一种情况，这两个过程一起会达到整体的最优目标，即使每一个单独都不能达到最优。

### 4.7 动态规划的效率

如果n 和 k 指代状态和动作的数量，那么策略的总数是k^n。DP方法保证能够在多项式时间内找到最优策略。

DP有时候被认为应用有限，因为 *维数灾难* ，状态的数量随着状态变量的增加成指数增长。 

在大状态空间的问题上，通常优先选择 *异步* DP方法。

### 4.8 总结

在这一章节中我们熟悉了动态规划的基本思想和算法，它可以用来解决有限MDP。

*略评估* 指的是（通常）迭代计算一个给定策略的价值函数。*策略提升* 指的是给定一个策略的价值函数计算一个提升的策略。 将这两种计算放在一起，就会得到策略迭代和价值迭代这两个最流行的DP方法。

他们根据其他估计更新估计数。我们把这种广义的思想叫做 *自助法*。















